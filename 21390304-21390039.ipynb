{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7944b64-a20f-46ef-bda3-218242ec9436",
   "metadata": {},
   "source": [
    "# ÎœÎ·Ï‡Î±Î½Î® Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ·Ï‚ Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±Ï‚\n",
    "### Î ÎµÏÎ¹Î³ÏÎ±Ï†Î®\n",
    "Î‘Ï…Ï„ÏŒ Ï„Î¿ Jupyter notebook ÎµÏ€Î¹Î´ÎµÎ¹ÎºÎ½ÏÎµÎ¹ Ï„Î· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î¼Î¹Î±Ï‚ Î¼Î·Ï‡Î±Î½Î®Ï‚ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€ÏŒ Ï„Î·Î½ Wikipedia Î¼Î­ÏƒÏ‰ Ï„Î¿Ï… API Ï„Î·Ï‚. ÎŸÎ¹ Ï‡ÏÎ®ÏƒÏ„ÎµÏ‚ Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Î±Î½Î±Î¶Î·Ï„Î®ÏƒÎ¿Ï…Î½ Î¬ÏÎ¸ÏÎ± ÎºÎ±Î¹ Î½Î± ÎµÏ†Î±ÏÎ¼ÏŒÏƒÎ¿Ï…Î½ Î´Î¹Î¬Ï†Î¿ÏÎ± Î¼Î¿Î½Ï„Î­Î»Î± Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚, ÏŒÏ€Ï‰Ï‚ Boolean, Vector Space Model (VSM) ÎºÎ±Î¹ BM25, Î³Î¹Î± Ï„Î·Î½ ÎµÏÏÎµÏƒÎ· Ï„Ï‰Î½ ÏƒÏ‡ÎµÏ„Î¹ÎºÏÎ½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½.\n",
    "\n",
    "### Î‘Î½Î±Î»Ï…Ï„Î¹ÎºÎ¬ Î’Î®Î¼Î±Ï„Î±:\n",
    "1. Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· Î¬ÏÎ¸ÏÏ‰Î½ Î±Ï€ÏŒ Ï„Î·Î½ Wikipedia.\n",
    "2. Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… ÎºÎ±Î¹ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚.\n",
    "3. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î±Î½Ï„Î¯ÏƒÏ„ÏÎ¿Ï†Î¿Ï… ÎµÏ…ÏÎµÏ„Î·ÏÎ¯Î¿Ï….\n",
    "4. Î•Ï†Î±ÏÎ¼Î¿Î³Î® Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ (Boolean, VSM, BM25).\n",
    "5. Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Ï„Î·Ï‚ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ Ï„Î¿Ï… ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cace061-fda4-498c-8c6b-5fcd002299ee",
   "metadata": {},
   "source": [
    "### @authors: Tilemachos Poulianas 21390304, Dionysia Giannopoulou 21390039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da39ed91-4410-4a05-af2e-183c7775fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a5cca-66d5-4f22-b2d9-730e71854bef",
   "metadata": {},
   "source": [
    "### Î•Î½Î´ÎµÎ¹ÎºÏ„Î¹ÎºÏŒ download Ï„Î¿Ï… nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09af4458-7f76-449b-9556-7a93c82fe2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tpoul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tpoul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tpoul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891dabf4-334f-48a3-bed7-c67bcc3aef14",
   "metadata": {},
   "source": [
    "## Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· Î†ÏÎ¸ÏÏ‰Î½ Î±Ï€ÏŒ Wikipedia\n",
    "Î— ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `fetch_wikipedia_articles` Î±Î½Î±ÎºÏ„Î¬ Î¬ÏÎ¸ÏÎ± Î±Ï€ÏŒ Ï„Î·Î½ Wikipedia Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿ API Ï„Î·Ï‚. ÎšÎ¬Î¸Îµ Î¬ÏÎ¸ÏÎ¿ Ï€Î¿Ï… Î±Î½Î±ÎºÏ„Î¬Ï„Î±Î¹ Î±Ï€Î¿Î¸Î·ÎºÎµÏÎµÏ„Î±Î¹ ÏƒÎµ Î­Î½Î± Î±ÏÏ‡ÎµÎ¯Î¿ JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47eed8c4-ba94-46f8-b9a5-febb88dc24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_articles(query, article_count):\n",
    "    \"\"\"\n",
    "    Fetches articles from Wikipedia using a search query and a specified number of results.\n",
    "    Automatically saves the articles to a JSON file named after the query.\n",
    "    \"\"\"\n",
    "    # Base URL for Wikipedia API\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    articles = []\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'search',\n",
    "        'srsearch': query,\n",
    "        'srlimit': article_count\n",
    "    }\n",
    "\n",
    "    # Use Wikipedia API to fetch articles\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get('query', {}).get('search', [])\n",
    "        for result in results:\n",
    "            title = result['title']\n",
    "            content = fetch_article_content(title)\n",
    "            if content:\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'content': preprocess_text(content)[:500],  # Limit to first 500 words\n",
    "                    'url': f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"  # Create URL\n",
    "                })\n",
    "\n",
    "    # Save the fetched articles to a JSON file\n",
    "    filename = f\"{query.replace(' ', '_')}.json\"\n",
    "    if articles:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Articles saved to '{filename}'.\")\n",
    "    else:\n",
    "        print(\"No articles fetched.\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "def fetch_article_content(title):\n",
    "    \"\"\"\n",
    "    Fetches the full content of a Wikipedia article given its title using the API.\n",
    "    \"\"\"\n",
    "    # Base URL for fetching article content\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'extracts',\n",
    "        'exintro': True,\n",
    "        'titles': title\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        pages = data['query']['pages']\n",
    "        page = next(iter(pages.values()))\n",
    "        content = page.get('extract', '')\n",
    "        if content:\n",
    "            content = clean_html(content)  # Clean HTML tags\n",
    "        return content\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d7dbd-74ed-4f8e-9320-8b43bcb6dae7",
   "metadata": {},
   "source": [
    "## Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÎšÎµÎ¹Î¼Î­Î½Î¿Ï… ÎºÎ±Î¹ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ HTML\n",
    "Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±Î¶ÏŒÎ¼Î±ÏƒÏ„Îµ Ï„Î± Î¬ÏÎ¸ÏÎ± Î¼Îµ Ï„Î¹Ï‚ ÏƒÏ…Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚ `preprocess_text` ÎºÎ±Î¹ `clean_html`. Î— `preprocess_text` Ï€ÏÎ±Î³Î¼Î±Ï„Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ÎºÎµÎ½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·, Î±Ï†Î±Î¯ÏÎµÏƒÎ· stop words ÎºÎ±Î¹ Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ® ÎµÏ†Î±ÏÎ¼Î¿Î³Î® stemming Î® lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42dd2ab4-10e8-470e-a0d4-3b9bf984f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(raw_text, use_stemming=False, use_lemmatization=False):\n",
    "    \"\"\"\n",
    "    Processes raw text: tokenization, stop-word removal, and optional stemming/lemmatization.\n",
    "    \"\"\"\n",
    "    # Load stop words for filtering\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(raw_text)  # Tokenize the raw text\n",
    "    cleaned_tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "\n",
    "    # Apply stemming if enabled\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        cleaned_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
    "\n",
    "    # Apply lemmatization if enabled\n",
    "    if use_lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        cleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Clean HTML\n",
    "def clean_html(raw_html):\n",
    "    \"\"\"\n",
    "    Removes HTML tags from raw HTML content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2da3a-4c82-4b31-a90a-320b6cb53835",
   "metadata": {},
   "source": [
    "## Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î‘Î½Ï„Î¯ÏƒÏ„ÏÎ¿Ï†Î¿Ï… Î•Ï…ÏÎµÏ„Î·ÏÎ¯Î¿Ï…\n",
    "Î— ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `create_inverted_index` Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Î­Î½Î± Î±Î½Ï„Î¯ÏƒÏ„ÏÎ¿Ï†Î¿ ÎµÏ…ÏÎµÏ„Î®ÏÎ¹Î¿ Î±Ï€ÏŒ Ï„Î·Î½ ÏƒÏ…Î»Î»Î¿Î³Î® Ï„Ï‰Î½ ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½. Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ Î±Î½Ï„Î¯ÏƒÏ„ÏÎ¿Ï†Î¿ ÎµÏ…ÏÎµÏ„Î®ÏÎ¹Î¿ Î³Î¹Î± Î½Î± Î±Ï€Î¿Î¸Î·ÎºÎµÏÏƒÎ¿Ï…Î¼Îµ Ï„Î± Î­Î³Î³ÏÎ±Ï†Î± Î±Î½Î¬ Î»Î­Î¾Î·, Ï€ÏÎ¿ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Î½Î± Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¾Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ Î±Î½Î±Î¶Î·Ï„Î®ÏƒÎµÎ¹Ï‚ Î¼Î±Ï‚ Î¼ÎµÎ¼Î¿Î½Ï‰Î¼Î­Î½Î± Î® ÏƒÏ…Î½Î´Ï…Î±ÏƒÎ¼Î­Î½Î±."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb381b2d-707a-412d-b248-abcfe22f534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(docs):\n",
    "    \"\"\"\n",
    "    Creates an inverted index from the document collection.\n",
    "    \"\"\"\n",
    "    # Initialize an empty inverted index using defaultdict\n",
    "    inverted_index = defaultdict(list)\n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        terms = doc['content'].split()  # Split document content into terms\n",
    "        for term in set(terms):\n",
    "            inverted_index[term].append(doc_id)  # Map each term to the document ID\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53969315-bd77-44e4-8768-6ef2fce2ab78",
   "metadata": {},
   "source": [
    "## Î•Ï†Î±ÏÎ¼Î¿Î³Î® ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚\n",
    "Î¥Î»Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ **Boolean**, **VSM** (Vector Space Model) ÎºÎ±Î¹ **BM25** Î³Î¹Î± Ï„Î· Î»Î®ÏˆÎ· Ï„Ï‰Î½ ÎºÎ±Î»ÏÏ„ÎµÏÏ‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ Î±Ï€ÏŒ Ï„Î± Î¬ÏÎ¸ÏÎ± Ï€Î¿Ï… Î±Î½Î±ÎºÏ„Î®Î½Î·Î¸Î·ÎºÎ±Î½."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c9f4169-0f55-45ae-aaf0-42aedd95caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_search(query, docs, inverted_index):\n",
    "    \"\"\"\n",
    "    Implements Boolean retrieval model with improved handling of NOT.\n",
    "    \"\"\"\n",
    "    query_terms = query.split()\n",
    "\n",
    "    # Initialize an empty set for the final result\n",
    "    matching_docs = set(range(len(docs)))\n",
    "\n",
    "    # Initialize a flag to track whether the next term should be negated\n",
    "    negate_next = False\n",
    "\n",
    "    # Initialize the logical operator to None\n",
    "    operator = None\n",
    "\n",
    "    for term in query_terms:\n",
    "        # Handle negation\n",
    "        if term == 'NOT':\n",
    "            negate_next = True\n",
    "            continue\n",
    "\n",
    "        # Check if the term is a logical operator\n",
    "        if term in ['AND', 'OR']:\n",
    "            operator = term\n",
    "            continue  # Skip logical operators\n",
    "\n",
    "        term_str = str(term)  # Convert term to a string\n",
    "\n",
    "        # Perform Boolean operations (AND, OR, NOT)\n",
    "        if term_str in inverted_index:\n",
    "            term_results = set(inverted_index[term_str])\n",
    "            if negate_next:\n",
    "                matching_docs -= term_results\n",
    "                negate_next = False\n",
    "            else:\n",
    "                # Apply logical operator\n",
    "                if operator == 'AND':\n",
    "                    matching_docs = matching_docs.intersection(term_results)\n",
    "                elif operator == 'OR':\n",
    "                    matching_docs = matching_docs.union(term_results)\n",
    "                else:\n",
    "                    matching_docs = term_results\n",
    "\n",
    "    # Return the documents matching the final result set\n",
    "    return [docs[doc_id] for doc_id in matching_docs]\n",
    "\n",
    "\n",
    "def vsm_search(query, docs):\n",
    "    \"\"\"\n",
    "    Implements Vector Space Model retrieval.\n",
    "    \"\"\"\n",
    "    # Convert documents into TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    doc_texts = [doc['content'] for doc in docs]\n",
    "    tfidf_matrix = vectorizer.fit_transform(doc_texts)\n",
    "\n",
    "    # Convert the query into a TF-IDF vector\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Rank documents based on cosine similarity\n",
    "    ranked_indices = cosine_similarities.argsort()[::-1]\n",
    "    ranked_docs = [docs[idx] for idx in ranked_indices if cosine_similarities[idx] > 0]\n",
    "\n",
    "    return ranked_docs\n",
    "\n",
    "def bm25(query, docs, k1=1.5, b=0.75):\n",
    "    \"\"\"\n",
    "    Implements BM25 model.\n",
    "    \"\"\"\n",
    "    # Calculate average document length for BM25\n",
    "    doc_lengths = [len(doc['content'].split()) for doc in docs]\n",
    "    avg_doc_length = sum(doc_lengths) / len(docs)\n",
    "\n",
    "    query_tokens = query.lower().split()  # Tokenize the query\n",
    "    scores = defaultdict(int)\n",
    "\n",
    "    for idx, doc in enumerate(docs):\n",
    "        doc_tokens = doc['content'].split()\n",
    "        doc_term_freq = defaultdict(int)\n",
    "        for token in doc_tokens:\n",
    "            doc_term_freq[token] += 1\n",
    "\n",
    "        for token in query_tokens:\n",
    "            # Calculate BM25 score components\n",
    "            idf = math.log((len(docs) - sum([1 for doc in docs if token in doc['content']]) + 0.5) / (sum([1 for doc in docs if token in doc['content']]) + 0.5) + 1.0)\n",
    "            tf = doc_term_freq[token] if token in doc_term_freq else 0\n",
    "            length_factor = (doc_lengths[idx] / avg_doc_length)\n",
    "            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * length_factor))\n",
    "            scores[idx] += score\n",
    "\n",
    "    # Rank documents based on BM25 scores\n",
    "    ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [docs[i] for i, _ in ranked_docs if _ > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90beec11-6f11-4455-b2fd-ae559e5963d8",
   "metadata": {},
   "source": [
    "## ÎœÎµÏ„ÏÎ¹ÎºÎ­Ï‚ Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚\n",
    "Î— Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î³Î¯Î½ÎµÏ„Î±Î¹ Î¼Î­ÏƒÏ‰ Î¼ÎµÏ„ÏÎ¹ÎºÏÎ½ **Precision**, **Recall**, **F1 Score** ÎºÎ±Î¹ **MAP**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cedd74f1-f202-4ef9-8795-a6620fbc46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(documents, results, query):\n",
    "    \"\"\"\n",
    "    Evaluate system using Precision, Recall, F1-Score, and MAP.\n",
    "    Dynamically limits the number of relevant documents to half the total retrieved documents.\n",
    "    \"\"\"\n",
    "    # Identify relevant documents based on the query\n",
    "    query_tokens = set(query.lower().split())\n",
    "    true_relevance = [1 if any(token in doc['content'] for token in query_tokens) else 0 for doc in documents]\n",
    "    retrieved_relevance = [1 if doc in results else 0 for doc in documents]\n",
    "\n",
    "    # Limit relevant documents to a specific number (e.g., half of retrieved)\n",
    "    max_relevant = len(results) // 2\n",
    "    true_relevance = [1 if idx < max_relevant else 0 for idx, val in enumerate(true_relevance)]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(true_relevance, retrieved_relevance, zero_division=0)\n",
    "    recall = recall_score(true_relevance, retrieved_relevance, zero_division=0)\n",
    "    f1 = f1_score(true_relevance, retrieved_relevance, zero_division=0)\n",
    "\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "    def mean_average_precision(true_relevance, retrieved_relevance):\n",
    "        relevant = 0\n",
    "        avg_precision = 0\n",
    "        for i, val in enumerate(retrieved_relevance):\n",
    "            if val == 1 and true_relevance[i] == 1:\n",
    "                relevant += 1\n",
    "                avg_precision += relevant / (i + 1)\n",
    "        return avg_precision / sum(true_relevance) if sum(true_relevance) > 0 else 0\n",
    "\n",
    "    map_score = mean_average_precision(true_relevance, retrieved_relevance)\n",
    "    print(f\"MAP: {map_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008a48f-2ecf-41aa-91fe-b1d9933c9508",
   "metadata": {},
   "source": [
    "## Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½\n",
    "Î— Ï€Î±ÏÎ¿Ï…ÏƒÎ¯Î±ÏƒÎ· Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ ÏƒÏ„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fd7dd04-f350-4e1a-90c1-51312a1fabea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_search_results(results):\n",
    "    \"\"\"\n",
    "    Outputs search results in a user-friendly format.\n",
    "    \"\"\"\n",
    "    if results:\n",
    "        for idx, result in enumerate(results, 1):\n",
    "            print(f\"{idx}. Title: {result['title']}\")\n",
    "            print(f\"Preview: {' '.join(result['content'].split()[:30])}...\\n\")\n",
    "    else:\n",
    "        print(\"No relevant documents found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8bdaf-beb8-486d-848f-72351f97f9f9",
   "metadata": {},
   "source": [
    "## Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Î‘Î½Î±Î¶Î·Ï„Î®ÏƒÎµÏ‰Î½\n",
    "Î— ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `execute_search` ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ ÏƒÏ„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î· Î½Î± Ï…Ï€Î¿Î²Î¬Î»ÎµÎ¹ Î±Î¹Ï„Î®Î¼Î±Ï„Î± Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚, Î½Î± ÎµÏ€Î¹Î»Î­Î¾ÎµÎ¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ ÎºÎ±Î¹ Î½Î± Î´ÎµÎ¹ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f88bc6-a68d-49ca-99eb-5ab82b95b2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query (or type 'exit' to quit):  sun NOT space\n",
      "Enter number of articles to fetch:  15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles saved to 'sun_NOT_space.json'.\n",
      "Fetched 15 documents.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose retrieval model (BOOLEAN, VSM, BM25):  BOOLEAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title: The House of the Rising Sun\n",
      "Preview: house rising sun traditional folk song sometimes called rising sun blues tells person life gone wrong city new orleans many versions also urge sibling parents children avoid fate successful commercial...\n",
      "\n",
      "2. Title: Elvis Presley\n",
      "Preview: elvis aaron presley january 8 1935 august 16 1977 known mononymously elvis american singer actor known king rock roll regarded one significant cultural figures 20th century presley energized performances interpretations...\n",
      "\n",
      "3. Title: Sun Tzu\n",
      "Preview: sun tzu chinese military general strategist philosopher writer lived eastern zhou period bc sun tzu traditionally credited author art war influential work military strategy affected western east asian philosophy military...\n",
      "\n",
      "4. Title: Ra\n",
      "Preview: ra ancient egyptian rêœ¥ also transliterated rêœ¥w ancient egyptian pronunciation ËˆÉ¾iËÊ•uw cuneiform ğ’Š‘ğ’€€ phoenician ğ¤“ğ¤ romanized rÊ¿ coptic â²£â² romanized rÄ“ ancient egyptian deity sun fifth dynasty 25th 24th centuries...\n",
      "\n",
      "5. Title: Aries (astrology)\n",
      "Preview: aries greek ÎºÏÎ¹ÏŒÏ‚ romanized kriÃ³s latin ariÄ“s lit first astrological sign zodiac spanning first 30 degrees celestial longitude Î» originates aries constellation tropical zodiac sun transits sign approximately march 21...\n",
      "\n",
      "6. Title: Louis XIV\n",
      "Preview: louis xiv 5 september 1638 1 september 1715 also known louis great louis le grand sun king le roi soleil king france 1643 death 1715 verified reign 72 years 110...\n",
      "\n",
      "7. Title: Chaz Bono\n",
      "Preview: chaz salvatore bono born chastity sun bono march 4 1969 american writer musician actor parents entertainers sonny bono cher became widely known appearances child television show sonny cher comedy hour...\n",
      "\n",
      "8. Title: Sun Belt Conference\n",
      "Preview: sun belt conference sbc collegiate athletic conference affiliated ncaa division since 1976 originally conference sun belt began sponsoring football 2001 football teams participate division football bowl subdivision fbs 14 member...\n",
      "\n",
      "9. Title: Sun Bowl\n",
      "Preview: sun bowl college football bowl game played since 1935 southwestern united states el paso texas along sugar bowl orange bowl bowl game country behind rose bowl usually held near end...\n",
      "\n",
      "10. Title: Arctic Circle\n",
      "Preview: arctic circle one two polar circles northernmost five major circles latitude shown maps earth 34 southern counterpart antarctic circle arctic circle marks southernmost latitude winter solstice northern hemisphere sun rise...\n",
      "\n",
      "11. Title: Aquarius (astrology)\n",
      "Preview: aquarius greek á½‘Î´ÏÎ¿Ï‡ÏŒÎ¿Ï‚ romanized hydrokhÃ³os latin eleventh astrological sign zodiac originating constellation aquarius tropical zodiac sun aquarius sign january 20 february 18 aquarius one three air signs alongside gemini libra...\n",
      "\n",
      "12. Title: Capricorn (astrology)\n",
      "Preview: capricorn greek Î±Î¹Î³ÏŒÎºÎµÏÏ‰Ï‚ romanized aigÃ³kerÅs latin horned goat tenth astrological sign zodiac twelve total zodiac signs originating constellation capricornus goat spans degree zodiac corresponding celestial longitude tropical zodiac sun transits...\n",
      "\n",
      "13. Title: Jeremy Clarkson\n",
      "Preview: jeremy charles robert clarkson born 11 april 1960 english television presenter journalist farmer author specialises motoring best known hosting television programmes top gear grand tour alongside richard hammond james may...\n",
      "\n",
      "Precision: 0.38\n",
      "Recall: 0.83\n",
      "F1-Score: 0.53\n",
      "MAP: 0.59\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query (or type 'exit' to quit):  car OR BMW \n",
      "Enter number of articles to fetch:  15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles saved to 'car_OR_BMW.json'.\n",
      "Fetched 15 documents.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose retrieval model (BOOLEAN, VSM, BM25):  BOOLEAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title: Mini (marque)\n",
      "Preview: mini stylised mini british automotive brand founded oxford 1969 owned german multinational automotive company bmw since 2000 used range small cars assembled united kingdom austria netherlands 16 february 2024 germany...\n",
      "\n",
      "2. Title: BMW 3 Series\n",
      "Preview: bmw 3 series line compact executive cars manufactured german automaker bmw since may 1975 successor 02 series produced seven generations first generation 3 series available saloon model range expanded include...\n",
      "\n",
      "3. Title: D-segment\n",
      "Preview: 4th category european segments passenger cars described large cars equivalent euro ncap large family car size class definition car category used north america compact executive cars part size category sales...\n",
      "\n",
      "4. Title: BMW 7 Series\n",
      "Preview: bmw 7 series luxury sedan manufactured marketed german automaker bmw since 1977 successor bmw e3 new six sedan seventh generation 7 series bmw flagship car available sedan bodystyle including long...\n",
      "\n",
      "5. Title: BMW M4\n",
      "Preview: bmw m4 version bmw 4 series automobile developed bmw motorsport division bmw built since 2014 part renumbering splits coupÃ© convertible variants 3 series 4 series m4 replaced variants bmw m3...\n",
      "\n",
      "6. Title: British Touring Car Championship\n",
      "Preview: kwik fit british touring car championship btcc touring car racing series held year united kingdom currently organised administered toca established 1958 british saloon car championship renamed british touring car championship...\n",
      "\n",
      "7. Title: BMW M1\n",
      "Preview: bmw m1 model code e26 sports car produced german automotive manufacturer bmw 1978 1981 late 1970s italian automobile manufacturer lamborghini entered agreement bmw build production racing car sufficient quantity homologation...\n",
      "\n",
      "8. Title: European Touring Car Championship\n",
      "Preview: european touring car championship international touring car racing series organised fia two incarnations first one 1963 1988 second 2000 2004 2005 superseded world touring car championship replaced european touring car...\n",
      "\n",
      "Precision: 0.12\n",
      "Recall: 0.25\n",
      "F1-Score: 0.17\n",
      "MAP: 0.08\n"
     ]
    }
   ],
   "source": [
    "def execute_search():\n",
    "    \"\"\"\n",
    "    Prompts user to perform dynamic searches.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Prompt user for query input\n",
    "        query = input(\"Enter your search query (or type 'exit' to quit): \").strip()\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Exiting search engine. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Prompt user for the number of articles to fetch\n",
    "            article_count = int(input(\"Enter number of articles to fetch: \").strip())\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number for the article count.\")\n",
    "            continue\n",
    "\n",
    "        # Fetch documents from Wikipedia\n",
    "        documents = fetch_wikipedia_articles(query=query, article_count=article_count)\n",
    "\n",
    "        if not documents:\n",
    "            print(\"No articles found. Try a different query.\")\n",
    "            continue\n",
    "\n",
    "        if len(documents) < article_count:\n",
    "            print(f\"Warning: Only {len(documents)} articles fetched instead of {article_count}.\")\n",
    "\n",
    "        print(f\"Fetched {len(documents)} documents.\")\n",
    "\n",
    "        # Create inverted index for the fetched documents\n",
    "        inverted_index = create_inverted_index(documents)\n",
    "\n",
    "        # Prompt user to choose a retrieval model\n",
    "        retrieval_method = input(\"Choose retrieval model (BOOLEAN, VSM, BM25): \").upper()\n",
    "        if retrieval_method == 'BOOLEAN':\n",
    "            results = boolean_search(query, documents, inverted_index)\n",
    "        elif retrieval_method == 'VSM':\n",
    "            results = vsm_search(query, documents)\n",
    "        elif retrieval_method == 'BM25':\n",
    "            results = bm25(query, documents)\n",
    "        else:\n",
    "            print(\"Invalid choice. Please select BOOLEAN, VSM, or BM25.\")\n",
    "            continue\n",
    "\n",
    "        # Display the search results to the user\n",
    "        display_search_results(results)\n",
    "\n",
    "        # Evaluate system performance based on results\n",
    "        evaluate_system(documents, results, query)\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    execute_search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
